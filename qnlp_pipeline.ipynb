{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58481856-3910-4675-9dce-8808b1957cbb",
   "metadata": {},
   "source": [
    "# The QNLP Pipeline for Twitter Sentiment Analysis\n",
    "## 1. Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb0fb825-03e9-41b5-9598-607cd61f3dd7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.9/13.9 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /opt/conda/lib/python3.9/site-packages (from en-core-web-sm==3.2.0) (3.2.4)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.1.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.3)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.64.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.8)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.17)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.7)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.27.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.9.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.2)\n",
      "Requirement already satisfied: click<8.1.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.9)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (61.2.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.21.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /opt/conda/lib/python3.9/site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2021.5.30)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.26.9)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.9/site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14f99bdd-0753-4dec-a784-ba0d1113519d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/app\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "from random import shuffle\n",
    "import re\n",
    "import spacy\n",
    "from discopy.tensor import Tensor\n",
    "from discopy import Word\n",
    "from discopy.rigid import Functor\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import random, unique\n",
    "from lambeq import AtomicType, IQPAnsatz, remove_cups, NumpyModel, spiders_reader\n",
    "from lambeq import BobcatParser, TreeReader, cups_reader, DepCCGParser\n",
    "from lambeq import Dataset\n",
    "from lambeq import QuantumTrainer, SPSAOptimizer\n",
    "from lambeq import TketModel\n",
    "from lambeq import SpacyTokeniser\n",
    "from pytket.extensions.qiskit import AerBackend\n",
    "\n",
    "\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.options.display.max_colwidth=80\n",
    "print(os.getcwd())\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "spacy.load('en_core_web_sm')\n",
    "\n",
    "BATCH_SIZE = 30\n",
    "EPOCHS = 100\n",
    "SEED = 0\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87093d7-c000-4b31-b0ae-4d210fad5d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sent_length(sent):\n",
    "    if type(sent) is not str:\n",
    "        return 9999999\n",
    "    word_list = sent.split(\" \")\n",
    "    return len(word_list)\n",
    "\n",
    "columns = [\"Id\",\"Entity\",\"Target\",\"Text\"]\n",
    "data = pd.read_csv(\"/app/data/twitter_training.csv\", names=columns,header=None)\n",
    "#data = data.sample(frac=1).reset_index(drop=True)\n",
    "data_val = pd.read_csv(\"/app/data/twitter_validation.csv\", names=columns,header=None)\n",
    "#data_val = data.sample(frac=1).reset_index(drop=True)\n",
    "df_train = data[[\"Text\",\"Target\"]]\n",
    "df_train = df_train.loc[(df_train[\"Target\"]=='Positive') | (df_train[\"Target\"]=='Negative') & (df_train[\"Text\"]!=np.nan)&(df_train[\"Text\"].map(get_sent_length)<=10)]\n",
    "df_train= df_train.drop_duplicates()\n",
    "df_val = data_val[['Text', 'Target']]\n",
    "df_val = df_val.loc[(df_val['Target'] == 'Positive') | (df_val['Target'] == 'Negative') & (df_val[\"Text\"]!=np.nan)&(df_val[\"Text\"].map(get_sent_length)<=10)]\n",
    "text_cleaning_re = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "def preprocess(text):\n",
    "    text = re.sub(text_cleaning_re, ' ', str(text).lower()).strip()\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(\"'\", \" \", text)\n",
    "    tokens = []\n",
    "    for token in text.split():\n",
    "            tokens.append(token)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"im\",\"i am\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"it's\",\"it is\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"you're\",\"you are\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"hasn't\",\"has not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"haven't\",\"have not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"don't\",\"do not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"doesn't\",\"does not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"won't\",\"will not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"shouldn't\",\"should not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"can't\",\"can not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"couldn't\",\"could not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"im\",\"i am\")\n",
    "df_val[\"Text\"]= df_val[\"Text\"].str.replace(\"it's\",\"it is\")\n",
    "df_val[\"Text\"]= df_val[\"Text\"].str.replace(\"you're\",\"you are\")\n",
    "df_val[\"Text\"]= df_val[\"Text\"].str.replace(\"hasn't\",\"has not\")\n",
    "df_val[\"Text\"]= df_val[\"Text\"].str.replace(\"haven't\",\"have not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"don't\",\"do not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"doesn't\",\"does not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"won't\",\"will not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"shouldn't\",\"should not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"can't\",\"can not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"couldn't\",\"could not\")\n",
    "df_train[\"Text\"] = df_train[\"Text\"].apply(preprocess)\n",
    "df_val[\"Text\"] = df_val[\"Text\"].apply(preprocess)\n",
    "df_train = df_train.dropna()\n",
    "df_val = df_val.dropna()\n",
    "\n",
    "negative_train_df = df_train.loc[df_train[\"Target\"]==\"Negative\"]\n",
    "positive_train_df = df_train.loc[df_train[\"Target\"]=='Positive']\n",
    "\n",
    "if len(positive_train_df)>=len(negative_train_df):\n",
    "    positive_train_df = positive_train_df.head(len(negative_train_df))\n",
    "else:\n",
    "    negative_train_df = negative_train_df.head(len(positive_train_df))\n",
    "\n",
    "negative_val_df = df_val.loc[df_val['Target'] == 'Negative']\n",
    "positive_val_df = df_val.loc[df_val['Target'] == 'Positive']\n",
    "\n",
    "if len(positive_val_df)>=len(negative_val_df):\n",
    "    positive_val_df = positive_val_df.head(len(negative_val_df))\n",
    "else:\n",
    "    negative_val_df = negative_val_df.head(len(positive_val_df))\n",
    "\n",
    "df_train = pd.concat([positive_train_df, negative_train_df])\n",
    "df_val = pd.concat([positive_val_df, negative_val_df])\n",
    "\n",
    "# Positive sentiment to [0,1], negative sentiment to [1,0]\n",
    "sentiment_train = []\n",
    "sentiment_val = []\n",
    "for i in df_train[\"Target\"]:\n",
    "    if i == \"Positive\":\n",
    "        sentiment_train.append([0,1])\n",
    "    else:\n",
    "        sentiment_train.append([1,0])\n",
    "\n",
    "df_train[\"Sentiment\"] = sentiment_train\n",
    "        \n",
    "for i in df_val[\"Target\"]:\n",
    "    if i == \"Positive\":\n",
    "        sentiment_val.append([0,1])\n",
    "    else:\n",
    "        sentiment_val.append([1,0])\n",
    "\n",
    "df_val[\"Sentiment\"] = sentiment_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bebbfb-7353-470f-bdf0-9e178cf5191a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02a2a73-5573-4449-9ffd-327520e3db9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786ac706-4f4e-427f-9081-f64867dcad41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcb21ae-7c17-496d-85bc-f8cb2164029c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16716bbd-9171-4df7-8923-49c6b5532a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x = \"Target\", data = df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7372026b-3e44-4738-81ac-56242d3285d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x = \"Target\", data = df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3361f377-0924-4315-b830-1fdaa314d722",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_all, train_label_all = df_train[\"Text\"].tolist(), df_train[\"Sentiment\"].tolist()\n",
    "dev_data, dev_labels = df_val[\"Text\"].tolist(), df_val[\"Sentiment\"].tolist()\n",
    "data = train_data_all+dev_data\n",
    "labels = train_label_all+dev_labels\n",
    "pairs = []\n",
    "for c in zip(labels, data):\n",
    "    if len(c[1]) != 0 and len(c[1].split(\" \"))<=5:\n",
    "        pairs.append(c)\n",
    "random.seed(0)\n",
    "random.shuffle(pairs)\n",
    "N_EXAMPLES = len(pairs)\n",
    "print(\"Total: {}\".format(N_EXAMPLES))\n",
    "TRAIN_RATIO_INDEX = 0.8\n",
    "TEST_RATIO_INDEX = TRAIN_RATIO_INDEX + 0.1\n",
    "DEV_RATIO_INDEX = TEST_RATIO_INDEX + 0.1\n",
    "train_labels, train_data = zip(*pairs[:round(N_EXAMPLES * TRAIN_RATIO_INDEX)])\n",
    "dev_labels, dev_data = zip(*pairs[round(N_EXAMPLES * TRAIN_RATIO_INDEX):round(N_EXAMPLES * TEST_RATIO_INDEX)])\n",
    "test_labels, test_data = zip(*pairs[round(N_EXAMPLES * TEST_RATIO_INDEX):round(N_EXAMPLES * DEV_RATIO_INDEX)])\n",
    "print(\"Data selected for train: {}\\nData selected for test: {}\\nData selected for dev: {}\".format(len(train_data), len(test_data), len(dev_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d7e01b-4b73-4acf-9f1a-54bdaddc956b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for replacing low occuring word(s) with <unk> token\n",
    "def replace(box):\n",
    "    if isinstance(box, Word) and dataset.count(box.name) < 1:\n",
    "        return Word('unk', box.cod, box.dom)\n",
    "    return box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d2a6d9-8f77-4339-a46d-c34ca92eeba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeniser = SpacyTokeniser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cd6016-9095-408a-adca-d166fe4cf101",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tokeniser.tokenise_sentences(train_data)\n",
    "dev_data = tokeniser.tokenise_sentences(dev_data)\n",
    "test_data = tokeniser.tokenise_sentences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad927eaf-74b7-4b53-9d24-16eb48296669",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_data)):\n",
    "    train_data[i] = ' '.join(train_data[i])\n",
    "\n",
    "for i in range(len(dev_data)):\n",
    "    dev_data[i] = ' '.join(dev_data[i])\n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    test_data[i] = ' '.join(test_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cc2042-2fcb-4f95-8840-b97d6b5e7503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training set words (with repetition)\n",
    "train_data_string = ' '.join(train_data)\n",
    "train_data_list = train_data_string.split(' ')\n",
    "# validation set words (with repetition)\n",
    "dev_data_string = ' '.join(dev_data)\n",
    "dev_data_list = dev_data_string.split(' ')\n",
    "# test set words (with repetition)\n",
    "test_data_string = ' '.join(test_data)\n",
    "test_data_list = test_data_string.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ec4902-3020-4bf1-9e82-eed7c721a42e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset words (with repetition)\n",
    "dataset = train_data_list + dev_data_list + test_data_list\n",
    "# list of all unique words in the dataset\n",
    "unique_words = unique(dataset)\n",
    "# frequency for each unique word\n",
    "counter = collections.Counter(dataset)\n",
    "#print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9090ef6f-2efc-4b60-b553-ea393eb589a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_functor = Functor(ob=lambda x: x, ar=replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c242b9-2adb-44f2-8424-4f72aedb3d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = BobcatParser(verbose='text')\n",
    "print(BobcatParser.available_models())\n",
    "#parser = spiders_reader\n",
    "#parser = DepCCGParser()\n",
    "#parser = cups_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9188794-34f3-4bf9-8cf1-85a96399a6b5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_train_diagrams = []\n",
    "new_train_labels = []\n",
    "raw_dev_diagrams = []\n",
    "new_dev_labels = []\n",
    "raw_test_diagrams = []\n",
    "new_test_labels = []\n",
    "for sent, label in zip(train_data, train_labels):\n",
    "    try:\n",
    "        diag = parser.sentence2diagram(sent)\n",
    "        raw_train_diagrams.append(diag)\n",
    "        new_train_labels.append(label)\n",
    "    except:\n",
    "        print(\"Cannot be parsed in train: {}\".format(sent))\n",
    "\n",
    "for sent, label in zip(dev_data, dev_labels):\n",
    "    try:\n",
    "        diag = parser.sentence2diagram(sent)\n",
    "        raw_dev_diagrams.append(diag)\n",
    "        new_dev_labels.append(label)\n",
    "    except:\n",
    "        print(\"Cannot be parsed in dev: {}\".format(sent))        \n",
    "\n",
    "\n",
    "for sent, label in zip(test_data, test_labels):\n",
    "    try:\n",
    "        diag = parser.sentence2diagram(sent)\n",
    "        raw_test_diagrams.append(diag)\n",
    "        new_test_labels.append(label)\n",
    "    except:\n",
    "        print(\"Cannot be parsed in test: {}\".format(sent))\n",
    "\n",
    "train_labels = new_train_labels\n",
    "dev_labels = new_dev_labels\n",
    "test_labels = new_test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4673107-10d8-4189-921c-c5700bd241c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tokenizing low occuring words in each dataset\n",
    "for i in range(len(raw_train_diagrams)):\n",
    "    raw_train_diagrams[i] = replace_functor(raw_train_diagrams[i])\n",
    "\n",
    "for i in range(len(raw_dev_diagrams)):\n",
    "    raw_dev_diagrams[i] = replace_functor(raw_dev_diagrams[i])\n",
    "\n",
    "for i in range(len(raw_test_diagrams)):\n",
    "    raw_test_diagrams[i] = replace_functor(raw_test_diagrams[i])\n",
    "\n",
    "# sample sentence diagram (entry 1)\n",
    "raw_train_diagrams[0].draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f1f761-e0e1-4f3f-b4b1-7b2d5741e028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging all diagrams into one for checking the new words\n",
    "raw_all_diagrams = raw_train_diagrams + raw_dev_diagrams + raw_test_diagrams\n",
    "\n",
    "# removing cups (after performing top-to-bottom scan of the word diagrams)\n",
    "train_diagrams = [remove_cups(diagram) for diagram in raw_train_diagrams]\n",
    "dev_diagrams = [remove_cups(diagram) for diagram in raw_dev_diagrams]\n",
    "test_diagrams = [remove_cups(diagram) for diagram in raw_test_diagrams]\n",
    "\n",
    "# sample sentence diagram (entry 1)\n",
    "train_diagrams[0].draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a586358b-ef10-4b21-b16e-d5d7146ccf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "ansatz = IQPAnsatz({AtomicType.NOUN: 1, AtomicType.SENTENCE: 1, AtomicType.PREPOSITIONAL_PHRASE: 1, AtomicType.NOUN_PHRASE:1, AtomicType.CONJUNCTION:1}, n_layers=1, n_single_qubit_params=3)\n",
    "\n",
    "# train/test circuits\n",
    "train_circuits = [ansatz(diagram) for diagram in train_diagrams]\n",
    "dev_circuits = [ansatz(diagram) for diagram in dev_diagrams]\n",
    "test_circuits = [ansatz(diagram) for diagram in test_diagrams]\n",
    "\n",
    "# sample circuit diagram\n",
    "train_circuits[0].draw(figsize=(9, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8aeec33-c119-401f-ab4c-1a46b6f26136",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_circuits = train_circuits + dev_circuits + test_circuits\n",
    "model = NumpyModel.from_diagrams(all_circuits, use_jit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cafa328-622d-4fec-8c33-31aabb097724",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = lambda y_hat, y: -np.sum(y * np.log(y_hat)) / len(y)  # binary cross-entropy loss\n",
    "acc = lambda y_hat, y: np.sum(np.round(y_hat) == y) / len(y) / 2  # half due to double-counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f39218-1d7f-4284-9912-2ec68655dad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = QuantumTrainer(\n",
    "    model,\n",
    "    loss_function=loss,\n",
    "    epochs=EPOCHS,\n",
    "    optimizer=SPSAOptimizer,\n",
    "    optim_hyperparams={'a': 0.2, 'c': 0.06, 'A':0.01*EPOCHS},\n",
    "    evaluate_functions={'acc': acc},\n",
    "    evaluate_on_train=True,\n",
    "    verbose = 'text',\n",
    "    seed=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3549cb78-ae37-4472-a7de-d843d91f2213",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(\n",
    "            train_circuits,\n",
    "            train_labels,\n",
    "            batch_size=BATCH_SIZE)\n",
    "\n",
    "val_dataset = Dataset(dev_circuits, dev_labels, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26254f62-4742-487d-ad50-a6d5f5bf7bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(train_dataset, val_dataset, logging_step=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b14e48f-7fe6-4fac-853f-878354b0897e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax_tl, ax_tr), (ax_bl, ax_br)) = plt.subplots(2, 2, sharex=True, sharey='row', figsize=(10, 6))\n",
    "ax_tl.set_title('Training set')\n",
    "ax_tr.set_title('Development set')\n",
    "ax_bl.set_xlabel('Iterations')\n",
    "ax_br.set_xlabel('Iterations')\n",
    "ax_bl.set_ylabel('Accuracy')\n",
    "ax_tl.set_ylabel('Loss')\n",
    "\n",
    "colours = iter(plt.rcParams['axes.prop_cycle'].by_key()['color'])\n",
    "ax_tl.plot(trainer.train_epoch_costs, color=next(colours))\n",
    "ax_bl.plot(trainer.train_results['acc'], color=next(colours))\n",
    "ax_tr.plot(trainer.val_costs, color=next(colours))\n",
    "ax_br.plot(trainer.val_results['acc'], color=next(colours))\n",
    "\n",
    "test_acc = acc(model(test_circuits), test_labels)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f8140e-e80a-46bd-b540-282fd9470335",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
