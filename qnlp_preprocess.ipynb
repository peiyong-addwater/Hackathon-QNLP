{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cf997fd-8f97-4317-bef6-16a8c5c6ebc0",
   "metadata": {},
   "source": [
    "# Preprocess data for QNLP pipeline\n",
    "Remove all the sentences that cannot be parsed by BobcatParser"
   ]
  },
  {
   "cell_type": "raw",
   "id": "696c52da-1b5e-4adf-88be-82fd75e3c98d",
   "metadata": {},
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d2fd85e-7513-40c6-b395-1d253060b594",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/peiyongw/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/peiyongw/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/peiyongw/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/peiyongw/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/peiyongw/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/peiyongw/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peiyongw/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/peiyongw/Desktop/Hackathon-QNLP\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "from random import shuffle\n",
    "import re\n",
    "import spacy\n",
    "from discopy.tensor import Tensor\n",
    "from discopy import Word\n",
    "from discopy.rigid import Functor\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import random, unique\n",
    "from lambeq import AtomicType, IQPAnsatz, remove_cups, NumpyModel, spiders_reader\n",
    "from lambeq import BobcatParser, TreeReader, cups_reader, DepCCGParser\n",
    "from lambeq import Dataset\n",
    "from lambeq import QuantumTrainer, SPSAOptimizer\n",
    "from lambeq import TketModel\n",
    "from lambeq import SpacyTokeniser\n",
    "from pytket.extensions.qiskit import AerBackend\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk import pos_tag, ne_chunk\n",
    "from nltk.chunk import tree2conlltags\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.options.display.max_colwidth=80\n",
    "print(os.getcwd())\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "spacy.load('en_core_web_sm')\n",
    "\n",
    "TOTAL_DATA_RATIO = 0.1 # only use part of the data\n",
    "MAX_LENGTH = 10 # only use short tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4624e0-410d-4635-9bef-f4bbead799ec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### **With stemming and lematization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5fed62-abb8-4e9d-9c76-bcaba7e66bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sent_length(sent):\n",
    "    if type(sent) is not str:\n",
    "        return 9999999999999\n",
    "    word_list = sent.split(\" \")\n",
    "    return len(word_list)\n",
    "\n",
    "columns = [\"Id\",\"Entity\",\"Target\",\"Text\"]\n",
    "data = pd.read_csv(os.path.join(os.getcwd(),\"data/twitter_training.csv\"), names=columns,header=None)\n",
    "#data = data.sample(frac=1).reset_index(drop=True)\n",
    "data_val = pd.read_csv(os.path.join(os.getcwd(), \"data/twitter_validation.csv\"), names=columns,header=None)\n",
    "#data_val = data.sample(frac=1).reset_index(drop=True)\n",
    "df_train = data[[\"Text\",\"Target\"]]\n",
    "df_train = df_train.loc[(df_train[\"Target\"]=='Positive') | (df_train[\"Target\"]=='Negative') & (df_train[\"Text\"]!=np.nan)&(df_train[\"Text\"].map(get_sent_length)<=MAX_LENGTH)]\n",
    "df_train= df_train.drop_duplicates()\n",
    "df_val = data_val[['Text', 'Target']]\n",
    "df_val = df_val.loc[(df_val['Target'] == 'Positive') | (df_val['Target'] == 'Negative') & (df_val[\"Text\"]!=np.nan)&(df_val[\"Text\"].map(get_sent_length)<=MAX_LENGTH)]\n",
    "text_cleaning_re = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "def preprocess(text):\n",
    "    text = re.sub(text_cleaning_re, ' ', str(text).lower()).strip()\n",
    "    \n",
    "    without_emoji = emoji_pattern.sub(r'', text)\n",
    "    \n",
    "    tokens = word_tokenize(str(without_emoji).replace(\"'\", \"\").lower()) \n",
    "    \n",
    "    # Remove Puncs\n",
    "    without_punc = [w for w in tokens if w.isalpha()]\n",
    "    \n",
    "    # Lemmatize\n",
    "    text_len = [WordNetLemmatizer().lemmatize(t) for t in without_punc]\n",
    "    # Stem\n",
    "    text_cleaned = [PorterStemmer().stem(w) for w in text_len]\n",
    "    \n",
    "    return \" \".join(text_cleaned)\n",
    "\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"im\",\"i am\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"i'm\",\"i am\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"I'm\",\"i am\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"it's\",\"it is\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"you're\",\"you are\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"hasn't\",\"has not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"haven't\",\"have not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"don't\",\"do not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"doesn't\",\"does not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"won't\",\"will not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"shouldn't\",\"should not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"can't\",\"can not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"couldn't\",\"could not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"im\",\"i am\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"i'm\",\"i am\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"I'm\",\"i am\")\n",
    "df_val[\"Text\"]= df_val[\"Text\"].str.replace(\"it's\",\"it is\")\n",
    "df_val[\"Text\"]= df_val[\"Text\"].str.replace(\"you're\",\"you are\")\n",
    "df_val[\"Text\"]= df_val[\"Text\"].str.replace(\"hasn't\",\"has not\")\n",
    "df_val[\"Text\"]= df_val[\"Text\"].str.replace(\"haven't\",\"have not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"don't\",\"do not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"doesn't\",\"does not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"won't\",\"will not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"shouldn't\",\"should not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"can't\",\"can not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"couldn't\",\"could not\")\n",
    "df_train[\"Text\"] = df_train[\"Text\"].apply(preprocess)\n",
    "df_val[\"Text\"] = df_val[\"Text\"].apply(preprocess)\n",
    "df_train = df_train.dropna()\n",
    "df_val = df_val.dropna()\n",
    "\n",
    "\n",
    "# Positive sentiment to [0,1], negative sentiment to [1,0]\n",
    "sentiment_train = []\n",
    "sentiment_val = []\n",
    "for i in df_train[\"Target\"]:\n",
    "    if i == \"Positive\":\n",
    "        sentiment_train.append([0,1])\n",
    "    else:\n",
    "        sentiment_train.append([1,0])\n",
    "\n",
    "df_train[\"Sentiment\"] = sentiment_train\n",
    "        \n",
    "for i in df_val[\"Target\"]:\n",
    "    if i == \"Positive\":\n",
    "        sentiment_val.append([0,1])\n",
    "    else:\n",
    "        sentiment_val.append([1,0])\n",
    "\n",
    "df_val[\"Sentiment\"] = sentiment_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb019d34-b2dc-447b-9712-9696a872e361",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf01964-3510-4b52-af89-38f5177e39fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fdbf5b-7501-404e-b04f-264ccea51ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1de2be1-bd00-47eb-9f77-590dfe775ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756015ac-04b7-44b6-bc24-350b01d6548c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x = \"Target\", data = df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28344647-d413-43a4-bc03-ff9b2daa1958",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x = \"Target\", data = df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cdb845-4028-4131-aac9-c28803880a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_all, train_label_all, train_target_all = df_train[\"Text\"].tolist(), df_train[\"Sentiment\"].tolist(), df_train['Target'].tolist()\n",
    "dev_data, dev_labels, dev_target = df_val[\"Text\"].tolist(), df_val[\"Sentiment\"].tolist(), df_val['Target'].tolist()\n",
    "data = train_data_all+dev_data\n",
    "labels = train_label_all+dev_labels\n",
    "targets = train_target_all+dev_target\n",
    "pairs = []\n",
    "for c in zip(labels, data, targets):\n",
    "    if len(c[1]) > 0:\n",
    "        pairs.append(c)\n",
    "random.seed(0)\n",
    "random.shuffle(pairs)\n",
    "N_EXAMPLES = len(pairs)\n",
    "print(\"Total: {}\".format(N_EXAMPLES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9aca13-740c-45ce-a0fb-3151ea717079",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = BobcatParser(verbose='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0229c291-d693-41c5-8374-f6fd3ffaac38",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_data = []\n",
    "new_label = []\n",
    "new_target = []\n",
    "i = 0 # positive\n",
    "j = 0 # negative\n",
    "for label, sent, target in tqdm(pairs):\n",
    "    try:\n",
    "        diag = parser.sentence2diagram(sent)\n",
    "    except:\n",
    "        pass\n",
    "    else:\n",
    "        sent_length = len(sent.split(\" \"))\n",
    "        if i>round(N_EXAMPLES*TOTAL_DATA_RATIO)//2 and j>round(N_EXAMPLES*TOTAL_DATA_RATIO)//2:\n",
    "            break\n",
    "        if target == \"Positive\" and i<=round(N_EXAMPLES*TOTAL_DATA_RATIO)//2:\n",
    "            new_data.append(sent)\n",
    "            new_label.append(label)\n",
    "            new_target.append(target)\n",
    "            i = i + 1\n",
    "        if target == 'Negative' and j<=round(N_EXAMPLES*TOTAL_DATA_RATIO)//2:\n",
    "            new_data.append(sent)\n",
    "            new_label.append(label)\n",
    "            new_target.append(target)\n",
    "            j = j + 1\n",
    "\n",
    "cleaned_qnlp_data = {\"data\":new_data, \"label\":new_label, \"target\":new_target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e118b9c-6d24-451b-b791-12ff00efacb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(cleaned_qnlp_data, open(\"cleaned_qnlp_data_stem_lematize.pkl\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b65b031-843a-4f18-96c6-cc0561a689c6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### **No stemming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7e1bad9-87d9-453b-9645-bbbd6b76e51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sent_length(sent):\n",
    "    if type(sent) is not str:\n",
    "        return 9999999999999\n",
    "    word_list = sent.split(\" \")\n",
    "    return len(word_list)\n",
    "\n",
    "columns = [\"Id\",\"Entity\",\"Target\",\"Text\"]\n",
    "data = pd.read_csv(os.path.join(os.getcwd(),\"data/twitter_training.csv\"), names=columns,header=None)\n",
    "#data = data.sample(frac=1).reset_index(drop=True)\n",
    "data_val = pd.read_csv(os.path.join(os.getcwd(), \"data/twitter_validation.csv\"), names=columns,header=None)\n",
    "#data_val = data.sample(frac=1).reset_index(drop=True)\n",
    "df_train = data[[\"Text\",\"Target\"]]\n",
    "df_train = df_train.loc[(df_train[\"Target\"]=='Positive') | (df_train[\"Target\"]=='Negative') & (df_train[\"Text\"]!=np.nan)&(df_train[\"Text\"].map(get_sent_length)<=MAX_LENGTH)]\n",
    "df_train= df_train.drop_duplicates()\n",
    "df_val = data_val[['Text', 'Target']]\n",
    "df_val = df_val.loc[(df_val['Target'] == 'Positive') | (df_val['Target'] == 'Negative') & (df_val[\"Text\"]!=np.nan)&(df_val[\"Text\"].map(get_sent_length)<=MAX_LENGTH)]\n",
    "text_cleaning_re = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "def preprocess(text):\n",
    "    text = re.sub(text_cleaning_re, ' ', str(text).lower()).strip()\n",
    "    \n",
    "    without_emoji = emoji_pattern.sub(r'', text)\n",
    "    \n",
    "    tokens = word_tokenize(str(without_emoji).replace(\"'\", \"\").lower()) \n",
    "    \n",
    "    # Remove Puncs\n",
    "    without_punc = [w for w in tokens if w.isalpha()]\n",
    "    \n",
    "    text_len = [WordNetLemmatizer().lemmatize(t) for t in without_punc]\n",
    "    \n",
    "    return \" \".join(text_len)\n",
    "\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"im\",\"i am\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"i'm\",\"i am\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"I'm\",\"i am\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"it's\",\"it is\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"you're\",\"you are\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"hasn't\",\"has not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"haven't\",\"have not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"don't\",\"do not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"doesn't\",\"does not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"won't\",\"will not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"shouldn't\",\"should not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"can't\",\"can not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"couldn't\",\"could not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"im\",\"i am\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"i'm\",\"i am\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"I'm\",\"i am\")\n",
    "df_val[\"Text\"]= df_val[\"Text\"].str.replace(\"it's\",\"it is\")\n",
    "df_val[\"Text\"]= df_val[\"Text\"].str.replace(\"you're\",\"you are\")\n",
    "df_val[\"Text\"]= df_val[\"Text\"].str.replace(\"hasn't\",\"has not\")\n",
    "df_val[\"Text\"]= df_val[\"Text\"].str.replace(\"haven't\",\"have not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"don't\",\"do not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"doesn't\",\"does not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"won't\",\"will not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"shouldn't\",\"should not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"can't\",\"can not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"couldn't\",\"could not\")\n",
    "df_train[\"Text\"] = df_train[\"Text\"].apply(preprocess)\n",
    "df_val[\"Text\"] = df_val[\"Text\"].apply(preprocess)\n",
    "df_train = df_train.dropna()\n",
    "df_val = df_val.dropna()\n",
    "\n",
    "\n",
    "# Positive sentiment to [0,1], negative sentiment to [1,0]\n",
    "sentiment_train = []\n",
    "sentiment_val = []\n",
    "for i in df_train[\"Target\"]:\n",
    "    if i == \"Positive\":\n",
    "        sentiment_train.append([0,1])\n",
    "    else:\n",
    "        sentiment_train.append([1,0])\n",
    "\n",
    "df_train[\"Sentiment\"] = sentiment_train\n",
    "        \n",
    "for i in df_val[\"Target\"]:\n",
    "    if i == \"Positive\":\n",
    "        sentiment_val.append([0,1])\n",
    "    else:\n",
    "        sentiment_val.append([1,0])\n",
    "\n",
    "df_val[\"Sentiment\"] = sentiment_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa8705ee-15f3-4f4e-ac20-f77974c72320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 25759\n"
     ]
    }
   ],
   "source": [
    "train_data_all, train_label_all, train_target_all = df_train[\"Text\"].tolist(), df_train[\"Sentiment\"].tolist(), df_train['Target'].tolist()\n",
    "dev_data, dev_labels, dev_target = df_val[\"Text\"].tolist(), df_val[\"Sentiment\"].tolist(), df_val['Target'].tolist()\n",
    "data = train_data_all+dev_data\n",
    "labels = train_label_all+dev_labels\n",
    "targets = train_target_all+dev_target\n",
    "pairs = []\n",
    "for c in zip(labels, data, targets):\n",
    "    if len(c[1]) > 0:\n",
    "        pairs.append(c)\n",
    "random.seed(0)\n",
    "random.shuffle(pairs)\n",
    "N_EXAMPLES = len(pairs)\n",
    "print(\"Total: {}\".format(N_EXAMPLES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28da51ba-74be-44f7-b6fd-4c52265a7615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a48373fc8b943fb968b7f268bc7df5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25759 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_data = []\n",
    "new_label = []\n",
    "new_target = []\n",
    "i = 0 # positive\n",
    "j = 0 # negative\n",
    "parser = BobcatParser(verbose='text')\n",
    "for label, sent, target in tqdm(pairs):\n",
    "    try:\n",
    "        diag = parser.sentence2diagram(sent)\n",
    "    except:\n",
    "        pass\n",
    "    else:\n",
    "        sent_length = len(sent.split(\" \"))\n",
    "        if i>round(N_EXAMPLES*TOTAL_DATA_RATIO)//2 and j>round(N_EXAMPLES*TOTAL_DATA_RATIO)//2:\n",
    "            break\n",
    "        if target == \"Positive\" and i<=round(N_EXAMPLES*TOTAL_DATA_RATIO)//2:\n",
    "            new_data.append(sent)\n",
    "            new_label.append(label)\n",
    "            new_target.append(target)\n",
    "            i = i + 1\n",
    "        if target == 'Negative' and j<=round(N_EXAMPLES*TOTAL_DATA_RATIO)//2:\n",
    "            new_data.append(sent)\n",
    "            new_label.append(label)\n",
    "            new_target.append(target)\n",
    "            j = j + 1\n",
    "        \n",
    "cleaned_qnlp_data = {\"data\":new_data, \"label\":new_label, \"target\":new_target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f18cf5c6-a1bd-4fab-8277-58d0f19e3c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(cleaned_qnlp_data, open(\"cleaned_qnlp_data_lematize.pkl\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6997ad74-aa25-4622-bd27-6a724982d394",
   "metadata": {},
   "source": [
    "### **Without stemming or lematization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f7a32eb-6539-4c87-841c-f03b21e2c9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sent_length(sent):\n",
    "    if type(sent) is not str:\n",
    "        return 9999999999999\n",
    "    word_list = sent.split(\" \")\n",
    "    return len(word_list)\n",
    "\n",
    "columns = [\"Id\",\"Entity\",\"Target\",\"Text\"]\n",
    "data = pd.read_csv(os.path.join(os.getcwd(),\"data/twitter_training.csv\"), names=columns,header=None)\n",
    "#data = data.sample(frac=1).reset_index(drop=True)\n",
    "data_val = pd.read_csv(os.path.join(os.getcwd(), \"data/twitter_validation.csv\"), names=columns,header=None)\n",
    "#data_val = data.sample(frac=1).reset_index(drop=True)\n",
    "df_train = data[[\"Text\",\"Target\"]]\n",
    "df_train = df_train.loc[(df_train[\"Target\"]=='Positive') | (df_train[\"Target\"]=='Negative') & (df_train[\"Text\"]!=np.nan)&(df_train[\"Text\"].map(get_sent_length)<=MAX_LENGTH)]\n",
    "df_train= df_train.drop_duplicates()\n",
    "df_val = data_val[['Text', 'Target']]\n",
    "df_val = df_val.loc[(df_val['Target'] == 'Positive') | (df_val['Target'] == 'Negative') & (df_val[\"Text\"]!=np.nan)&(df_val[\"Text\"].map(get_sent_length)<=MAX_LENGTH)]\n",
    "text_cleaning_re = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "def preprocess(text):\n",
    "    text = re.sub(text_cleaning_re, ' ', str(text).lower()).strip()\n",
    "    \n",
    "    without_emoji = emoji_pattern.sub(r'', text)\n",
    "    \n",
    "    tokens = word_tokenize(str(without_emoji).replace(\"'\", \"\").lower()) \n",
    "    \n",
    "    # Remove Puncs\n",
    "    without_punc = [w for w in tokens if w.isalpha()]\n",
    "    \n",
    "    # text_len = [WordNetLemmatizer().lemmatize(t) for t in without_punc]\n",
    "    \n",
    "    return \" \".join(without_punc)\n",
    "\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"im\",\"i am\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"i'm\",\"i am\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"I'm\",\"i am\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"it's\",\"it is\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"you're\",\"you are\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"hasn't\",\"has not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"haven't\",\"have not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"don't\",\"do not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"doesn't\",\"does not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"won't\",\"will not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"shouldn't\",\"should not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"can't\",\"can not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"couldn't\",\"could not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"im\",\"i am\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"i'm\",\"i am\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"I'm\",\"i am\")\n",
    "df_val[\"Text\"]= df_val[\"Text\"].str.replace(\"it's\",\"it is\")\n",
    "df_val[\"Text\"]= df_val[\"Text\"].str.replace(\"you're\",\"you are\")\n",
    "df_val[\"Text\"]= df_val[\"Text\"].str.replace(\"hasn't\",\"has not\")\n",
    "df_val[\"Text\"]= df_val[\"Text\"].str.replace(\"haven't\",\"have not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"don't\",\"do not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"doesn't\",\"does not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"won't\",\"will not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"shouldn't\",\"should not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"can't\",\"can not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"couldn't\",\"could not\")\n",
    "df_train[\"Text\"] = df_train[\"Text\"].apply(preprocess)\n",
    "df_val[\"Text\"] = df_val[\"Text\"].apply(preprocess)\n",
    "df_train = df_train.dropna()\n",
    "df_val = df_val.dropna()\n",
    "\n",
    "\n",
    "# Positive sentiment to [0,1], negative sentiment to [1,0]\n",
    "sentiment_train = []\n",
    "sentiment_val = []\n",
    "for i in df_train[\"Target\"]:\n",
    "    if i == \"Positive\":\n",
    "        sentiment_train.append([0,1])\n",
    "    else:\n",
    "        sentiment_train.append([1,0])\n",
    "\n",
    "df_train[\"Sentiment\"] = sentiment_train\n",
    "        \n",
    "for i in df_val[\"Target\"]:\n",
    "    if i == \"Positive\":\n",
    "        sentiment_val.append([0,1])\n",
    "    else:\n",
    "        sentiment_val.append([1,0])\n",
    "\n",
    "df_val[\"Sentiment\"] = sentiment_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23ed0cd2-f297-461e-bb0f-d282b0e83dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 25759\n"
     ]
    }
   ],
   "source": [
    "train_data_all, train_label_all, train_target_all = df_train[\"Text\"].tolist(), df_train[\"Sentiment\"].tolist(), df_train['Target'].tolist()\n",
    "dev_data, dev_labels, dev_target = df_val[\"Text\"].tolist(), df_val[\"Sentiment\"].tolist(), df_val['Target'].tolist()\n",
    "data = train_data_all+dev_data\n",
    "labels = train_label_all+dev_labels\n",
    "targets = train_target_all+dev_target\n",
    "pairs = []\n",
    "for c in zip(labels, data, targets):\n",
    "    if len(c[1]) > 0:\n",
    "        pairs.append(c)\n",
    "random.seed(0)\n",
    "random.shuffle(pairs)\n",
    "N_EXAMPLES = len(pairs)\n",
    "print(\"Total: {}\".format(N_EXAMPLES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e799c02-c0ad-44c5-9f7e-5ea658b165ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e230a1badb414bd28030b1ae84353e92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25759 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_data = []\n",
    "new_label = []\n",
    "new_target = []\n",
    "i = 0 # positive\n",
    "j = 0 # negative\n",
    "parser = BobcatParser(verbose='text')\n",
    "for label, sent, target in tqdm(pairs):\n",
    "    try:\n",
    "        diag = parser.sentence2diagram(sent)\n",
    "    except:\n",
    "        pass\n",
    "    else:\n",
    "        sent_length = len(sent.split(\" \"))\n",
    "        if i>round(N_EXAMPLES*TOTAL_DATA_RATIO)//2 and j>round(N_EXAMPLES*TOTAL_DATA_RATIO)//2:\n",
    "            break\n",
    "        if target == \"Positive\" and i<=round(N_EXAMPLES*TOTAL_DATA_RATIO)//2:\n",
    "            new_data.append(sent)\n",
    "            new_label.append(label)\n",
    "            new_target.append(target)\n",
    "            i = i + 1\n",
    "        if target == 'Negative' and j<=round(N_EXAMPLES*TOTAL_DATA_RATIO)//2:\n",
    "            new_data.append(sent)\n",
    "            new_label.append(label)\n",
    "            new_target.append(target)\n",
    "            j = j + 1\n",
    "\n",
    "cleaned_qnlp_data = {\"data\":new_data, \"label\":new_label, \"target\":new_target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496963db-a3b9-49f2-826c-b33b7c3ee98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(cleaned_qnlp_data, open(\"cleaned_qnlp_data.pkl\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c111ac-f335-4118-a36c-c4882fc2dadd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
