{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cf997fd-8f97-4317-bef6-16a8c5c6ebc0",
   "metadata": {},
   "source": [
    "# Preprocess data for QNLP pipeline\n",
    "Remove all the sentences that cannot be parsed by BobcatParser"
   ]
  },
  {
   "cell_type": "raw",
   "id": "696c52da-1b5e-4adf-88be-82fd75e3c98d",
   "metadata": {},
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d2fd85e-7513-40c6-b395-1d253060b594",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/peiyongw/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/peiyongw/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/peiyongw/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/peiyongw/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/peiyongw/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/peiyongw/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peiyongw/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/peiyongw/Desktop/Hackathon-QNLP\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "from random import shuffle\n",
    "import re\n",
    "import spacy\n",
    "from discopy.tensor import Tensor\n",
    "from discopy import Word\n",
    "from discopy.rigid import Functor\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import random, unique\n",
    "from lambeq import AtomicType, IQPAnsatz, remove_cups, NumpyModel, spiders_reader\n",
    "from lambeq import BobcatParser, TreeReader, cups_reader, DepCCGParser\n",
    "from lambeq import Dataset\n",
    "from lambeq import QuantumTrainer, SPSAOptimizer\n",
    "from lambeq import TketModel\n",
    "from lambeq import SpacyTokeniser\n",
    "from pytket.extensions.qiskit import AerBackend\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk import pos_tag, ne_chunk\n",
    "from nltk.chunk import tree2conlltags\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.options.display.max_colwidth=80\n",
    "print(os.getcwd())\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "spacy.load('en_core_web_sm')\n",
    "\n",
    "TOTAL_DATA_RATIO = 0.1 # only use part of the data\n",
    "MAX_LENGTH = 10 # only use short tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4624e0-410d-4635-9bef-f4bbead799ec",
   "metadata": {},
   "source": [
    "**With stemming and lematization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b5fed62-abb8-4e9d-9c76-bcaba7e66bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sent_length(sent):\n",
    "    if type(sent) is not str:\n",
    "        return 9999999999999\n",
    "    word_list = sent.split(\" \")\n",
    "    return len(word_list)\n",
    "\n",
    "columns = [\"Id\",\"Entity\",\"Target\",\"Text\"]\n",
    "data = pd.read_csv(os.path.join(os.getcwd(),\"data/twitter_training.csv\"), names=columns,header=None)\n",
    "#data = data.sample(frac=1).reset_index(drop=True)\n",
    "data_val = pd.read_csv(os.path.join(os.getcwd(), \"data/twitter_validation.csv\"), names=columns,header=None)\n",
    "#data_val = data.sample(frac=1).reset_index(drop=True)\n",
    "df_train = data[[\"Text\",\"Target\"]]\n",
    "df_train = df_train.loc[(df_train[\"Target\"]=='Positive') | (df_train[\"Target\"]=='Negative') & (df_train[\"Text\"]!=np.nan)&(df_train[\"Text\"].map(get_sent_length)<=MAX_LENGTH)]\n",
    "df_train= df_train.drop_duplicates()\n",
    "df_val = data_val[['Text', 'Target']]\n",
    "df_val = df_val.loc[(df_val['Target'] == 'Positive') | (df_val['Target'] == 'Negative') & (df_val[\"Text\"]!=np.nan)&(df_val[\"Text\"].map(get_sent_length)<=MAX_LENGTH)]\n",
    "text_cleaning_re = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "def preprocess(text):\n",
    "    text = re.sub(text_cleaning_re, ' ', str(text).lower()).strip()\n",
    "    \n",
    "    without_emoji = emoji_pattern.sub(r'', text)\n",
    "    \n",
    "    tokens = word_tokenize(str(without_emoji).replace(\"'\", \"\").lower()) \n",
    "    \n",
    "    # Remove Puncs\n",
    "    without_punc = [w for w in tokens if w.isalpha()]\n",
    "    \n",
    "    # Lemmatize\n",
    "    text_len = [WordNetLemmatizer().lemmatize(t) for t in without_punc]\n",
    "    # Stem\n",
    "    text_cleaned = [PorterStemmer().stem(w) for w in text_len]\n",
    "    \n",
    "    return \" \".join(text_cleaned)\n",
    "\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"im\",\"i am\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"i'm\",\"i am\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"I'm\",\"i am\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"it's\",\"it is\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"you're\",\"you are\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"hasn't\",\"has not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"haven't\",\"have not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"don't\",\"do not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"doesn't\",\"does not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"won't\",\"will not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"shouldn't\",\"should not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"can't\",\"can not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"couldn't\",\"could not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"im\",\"i am\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"i'm\",\"i am\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"I'm\",\"i am\")\n",
    "df_val[\"Text\"]= df_val[\"Text\"].str.replace(\"it's\",\"it is\")\n",
    "df_val[\"Text\"]= df_val[\"Text\"].str.replace(\"you're\",\"you are\")\n",
    "df_val[\"Text\"]= df_val[\"Text\"].str.replace(\"hasn't\",\"has not\")\n",
    "df_val[\"Text\"]= df_val[\"Text\"].str.replace(\"haven't\",\"have not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"don't\",\"do not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"doesn't\",\"does not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"won't\",\"will not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"shouldn't\",\"should not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"can't\",\"can not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"couldn't\",\"could not\")\n",
    "df_train[\"Text\"] = df_train[\"Text\"].apply(preprocess)\n",
    "df_val[\"Text\"] = df_val[\"Text\"].apply(preprocess)\n",
    "df_train = df_train.dropna()\n",
    "df_val = df_val.dropna()\n",
    "\n",
    "\n",
    "# Positive sentiment to [0,1], negative sentiment to [1,0]\n",
    "sentiment_train = []\n",
    "sentiment_val = []\n",
    "for i in df_train[\"Target\"]:\n",
    "    if i == \"Positive\":\n",
    "        sentiment_train.append([0,1])\n",
    "    else:\n",
    "        sentiment_train.append([1,0])\n",
    "\n",
    "df_train[\"Sentiment\"] = sentiment_train\n",
    "        \n",
    "for i in df_val[\"Target\"]:\n",
    "    if i == \"Positive\":\n",
    "        sentiment_val.append([0,1])\n",
    "    else:\n",
    "        sentiment_val.append([1,0])\n",
    "\n",
    "df_val[\"Sentiment\"] = sentiment_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb019d34-b2dc-447b-9712-9696a872e361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 25483 entries, 0 to 74681\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   Text       25483 non-null  object\n",
      " 1   Target     25483 non-null  object\n",
      " 2   Sentiment  25483 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 796.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaf01964-3510-4b52-af89-38f5177e39fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 354 entries, 6 to 998\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   Text       354 non-null    object\n",
      " 1   Target     354 non-null    object\n",
      " 2   Sentiment  354 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 11.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df_val.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52fdbf5b-7501-404e-b04f-264ccea51ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Target</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i am get on borderland and i will murder you all</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i am come to the border and i will kill you all</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i am get on borderland and i will kill you all</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am come on borderland and i will murder you all</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am get on borderland and i will murder you me all</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text    Target Sentiment\n",
       "0     i am get on borderland and i will murder you all  Positive    [0, 1]\n",
       "1      i am come to the border and i will kill you all  Positive    [0, 1]\n",
       "2       i am get on borderland and i will kill you all  Positive    [0, 1]\n",
       "3    i am come on borderland and i will murder you all  Positive    [0, 1]\n",
       "4  i am get on borderland and i will murder you me all  Positive    [0, 1]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1de2be1-bd00-47eb-9f77-590dfe775ad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Target</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>thank you eamaddennfl new te austin hooper in the orang brown brown pic twit...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rocket leagu sea of thief or rainbow six sieg i love play all three on strea...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>my as still knee deep in assassin creed odyssey with no way out anyti ame so...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>the profession dota scene is fuck explod and i complet welcom it get the gar...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>itch to assassin tccgif assassinscreedblackflag assassinscre thecapturedcoll...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                               Text    Target Sentiment\n",
       "6   thank you eamaddennfl new te austin hooper in the orang brown brown pic twit...  Positive    [0, 1]\n",
       "7   rocket leagu sea of thief or rainbow six sieg i love play all three on strea...  Positive    [0, 1]\n",
       "8   my as still knee deep in assassin creed odyssey with no way out anyti ame so...  Positive    [0, 1]\n",
       "10  the profession dota scene is fuck explod and i complet welcom it get the gar...  Positive    [0, 1]\n",
       "11  itch to assassin tccgif assassinscreedblackflag assassinscre thecapturedcoll...  Positive    [0, 1]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "756015ac-04b7-44b6-bc24-350b01d6548c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Target', ylabel='count'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x = \"Target\", data = df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28344647-d413-43a4-bc03-ff9b2daa1958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Target', ylabel='count'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEGCAYAAACKB4k+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAR0UlEQVR4nO3de7DcZX3H8fcHwTsomIgYsHEwjoNWYzmleGml2kGl06JWEeoFlWmsg1qqdQadtqJTW1pBi1Kp8QZ0VASv0VoQ0dZLRQ0YQwAdU4WSNEBEK6B4SfrtH/uchyWchE3Inj3Jeb9mfrPP7/ndvpvZ5JPfZZ9NVSFJEsAeky5AkjR3GAqSpM5QkCR1hoIkqTMUJEndnpMu4O5YsGBBLV68eNJlSNIu5bLLLvthVS2cadkuHQqLFy9m5cqVky5DknYpSa7d2jIvH0mSOkNBktQZCpKkzlCQJHWGgiSpMxQkSZ2hIEnqDAVJUmcoSJK6XfobzTvDoa87d9IlaA667K0vnnQJ0kR4piBJ6gwFSVJnKEiSOkNBktQZCpKkzlCQJHWGgiSpMxQkSZ2hIEnqDAVJUmcoSJI6Q0GS1BkKkqTOUJAkdYaCJKkzFCRJ3dhCIclBSb6Y5KokVyb5s9Z/SpL1SVa16aihbV6fZG2S7yZ5+rhqkyTNbJy/vLYJeG1VXZ5kb+CyJBe3ZW+vqtOGV05yCHAs8GjgocDnkzyyqjaPsUZJ0pCxnSlU1Yaqury1bwGuBhZtY5OjgfOq6hdV9QNgLXDYuOqTJN3ZrNxTSLIYeDzw9db1yiSrk7w/yb6tbxFw3dBm65ghRJIsS7IyycqNGzeOs2xJmnfGHgpJ7g98DDipqm4GzgIOBpYCG4DTt2d/VbW8qqaqamrhwoU7u1xJmtfGGgpJ9mIQCB+sqo8DVNUNVbW5qv4PeA+3XyJaDxw0tPmBrU+SNEvG+fRRgPcBV1fV24b6Dxha7dnAmtZeARyb5F5JHg4sAb4xrvokSXc2zqePngS8CLgiyarW9wbguCRLgQKuAV4OUFVXJjkfuIrBk0sn+uSRJM2usYVCVX0FyAyLPruNbd4CvGVcNUmSts1vNEuSOkNBktQZCpKkzlCQJHWGgiSpMxQkSZ2hIEnqDAVJUmcoSJI6Q0GS1BkKkqTOUJAkdYaCJKkzFCRJnaEgSeoMBUlSZyhIkjpDQZLUGQqSpM5QkCR1hoIkqTMUJEmdoSBJ6gwFSVJnKEiSOkNBktQZCpKkzlCQJHVjC4UkByX5YpKrklyZ5M9a/35JLk7yvfa6b+tPknckWZtkdZLfGFdtkqSZjfNMYRPw2qo6BDgcODHJIcDJwCVVtQS4pM0DPBNY0qZlwFljrE2SNIOxhUJVbaiqy1v7FuBqYBFwNHBOW+0c4FmtfTRwbg1cCjwwyQHjqk+SdGezck8hyWLg8cDXgf2rakNbdD2wf2svAq4b2mxd69tyX8uSrEyycuPGjeMrWpLmobGHQpL7Ax8DTqqqm4eXVVUBtT37q6rlVTVVVVMLFy7ciZVKksYaCkn2YhAIH6yqj7fuG6YvC7XXG1v/euCgoc0PbH2SpFkyzqePArwPuLqq3ja0aAVwfGsfD3xqqP/F7Smkw4GfDF1mkiTNgj3HuO8nAS8CrkiyqvW9ATgVOD/JCcC1wDFt2WeBo4C1wM+Al46xNknSDMYWClX1FSBbWfy0GdYv4MRx1SNJumt+o1mS1BkKkqTOUJAkdYaCJKkzFCRJnaEgSeoMBUlSZyhIkjpDQZLUGQqSpM5QkCR1hoIkqTMUJEmdoSBJ6gwFSVJnKEiSOkNBktQZCpKkzlCQJHWGgiSpMxQkSZ2hIEnqDAVJUmcoSJK6kUIhySWj9EmSdm17bmthknsD9wUWJNkXSFu0D7BozLVJkmbZNkMBeDlwEvBQ4DJuD4WbgTPHV5YkaRK2GQpVdQZwRpJXVdU7Z6kmSdKEjHRPoaremeSJSf44yYunp21tk+T9SW5Msmao75Qk65OsatNRQ8ten2Rtku8mefqOvyVJ0o66q8tHACT5F+BgYBWwuXUXcO42NjubwSWmLdd5e1WdtsX+DwGOBR7N4FLV55M8sqo2I0maNSOFAjAFHFJVNeqOq+pLSRaPuPrRwHlV9QvgB0nWAocBXxv1eJKku2/U7ymsAR6yk475yiSr2+WlfVvfIuC6oXXW4dNNkjTrRg2FBcBVSS5KsmJ62oHjncXgMtRSYANw+vbuIMmyJCuTrNy4ceMOlCBJ2ppRLx+dsjMOVlU3TLeTvAf4TJtdDxw0tOqBrW+mfSwHlgNMTU2NfDlLknTXRgqFqvqPnXGwJAdU1YY2+2wGl6UAVgAfSvI2BjealwDf2BnHlCSNbtSnj25h8LQRwD2BvYCfVtU+29jmw8ARDL4NvQ54I3BEkqVtX9cw+HIcVXVlkvOBq4BNwIk+eSRJs2/UM4W9p9tJwuBpocPvYpvjZuh+3zbWfwvwllHqkSSNx3aPkloDnwT8gpkk7WZGvXz0nKHZPRh8b+HnY6lIkjQxoz599AdD7U0M7gccvdOrkSRN1Kj3FF467kIkSZM36o/sHJjkE22AuxuTfCzJgeMuTpI0u0a90fwBBt8leGibPt36JEm7kVFDYWFVfaCqNrXpbGDhGOuSJE3AqKFwU5IXJrlHm14I3DTOwiRJs2/UUHgZcAxwPYOB7J4LvGRMNUmSJmTUR1LfDBxfVT8GSLIfcBqDsJAk7SZGPVN47HQgAFTVj4DHj6ckSdKkjBoKewz9IM70mcKoZxmSpF3EqP+wnw58LckFbf55OHidJO12Rv1G87lJVgJPbV3PqaqrxleWJGkSRr4E1ELAIJCk3dh2D50tSdp9GQqSpM5QkCR1hoIkqTMUJEmdoSBJ6gwFSVJnKEiSOkNBktQZCpKkzlCQJHWGgiSpMxQkSZ2hIEnqxhYKSd6f5MYka4b69ktycZLvtdd9W3+SvCPJ2iSrk/zGuOqSJG3dOM8UzgaesUXfycAlVbUEuKTNAzwTWNKmZcBZY6xLkrQVYwuFqvoS8KMtuo8Gzmntc4BnDfWfWwOXAg9McsC4apMkzWy27ynsX1UbWvt6YP/WXgRcN7TeutZ3J0mWJVmZZOXGjRvHV6kkzUMTu9FcVQXUDmy3vKqmqmpq4cKFY6hMkuav2Q6FG6YvC7XXG1v/euCgofUObH2SpFk026GwAji+tY8HPjXU/+L2FNLhwE+GLjNJkmbJnuPacZIPA0cAC5KsA94InAqcn+QE4FrgmLb6Z4GjgLXAz4CXjqsuSdLWjS0Uquq4rSx62gzrFnDiuGqRJI3GbzRLkjpDQZLUGQqSpM5QkCR1hoIkqTMUJEmdoSBJ6gwFSVJnKEiSOkNBktQZCpKkbmxjH0m6e/77zb8+6RI0Bz3sr68Y6/49U5AkdYaCJKkzFCRJnaEgSeoMBUlSZyhIkjpDQZLUGQqSpM5QkCR1hoIkqTMUJEmdoSBJ6gwFSVJnKEiSOkNBktQZCpKkbiI/spPkGuAWYDOwqaqmkuwHfARYDFwDHFNVP55EfZI0X03yTOF3q2ppVU21+ZOBS6pqCXBJm5ckzaK5dPnoaOCc1j4HeNbkSpGk+WlSoVDA55JclmRZ69u/qja09vXA/jNtmGRZkpVJVm7cuHE2apWkeWMi9xSAJ1fV+iQPBi5O8p3hhVVVSWqmDatqObAcYGpqasZ1JEk7ZiJnClW1vr3eCHwCOAy4IckBAO31xknUJknz2ayHQpL7Jdl7ug0cCawBVgDHt9WOBz4127VJ0nw3ictH+wOfSDJ9/A9V1YVJvgmcn+QE4FrgmAnUJknz2qyHQlV9H3jcDP03AU+b7XokSbebS4+kSpImzFCQJHWGgiSpMxQkSZ2hIEnqDAVJUmcoSJI6Q0GS1BkKkqTOUJAkdYaCJKkzFCRJnaEgSeoMBUlSZyhIkjpDQZLUGQqSpM5QkCR1hoIkqTMUJEmdoSBJ6gwFSVJnKEiSOkNBktQZCpKkzlCQJHWGgiSpMxQkSZ2hIEnq5lwoJHlGku8mWZvk5EnXI0nzyZwKhST3AP4JeCZwCHBckkMmW5UkzR9zKhSAw4C1VfX9qvolcB5w9IRrkqR5Y89JF7CFRcB1Q/PrgN8aXiHJMmBZm701yXdnqbb5YAHww0kXMRfktOMnXYLuyM/mtDdmZ+zl17a2YK6Fwl2qquXA8knXsTtKsrKqpiZdh7QlP5uzZ65dPloPHDQ0f2DrkyTNgrkWCt8EliR5eJJ7AscCKyZckyTNG3Pq8lFVbUrySuAi4B7A+6vqygmXNZ94WU5zlZ/NWZKqmnQNkqQ5Yq5dPpIkTZChIEnqDIXdQJLNSVYlWZPkgiT33c7tH5rko629NMlRQ8v+0OFGtD2SVJLTh+b/IskpYzjOG7aY/8+dfYz5yFDYPdxWVUur6jHAL4E/3Z6Nq+p/quq5bXYpcNTQshVVdepOq1TzwS+A5yRZMObj3CEUquqJYz7evGAo7H6+DDwiyX5JPplkdZJLkzwWIMlT2lnFqiTfSrJ3ksXtLOOewJuB57flz0/ykiRnJnlAkmuT7NH2c78k1yXZK8nBSS5MclmSLyd51ATfvyZvE4Onhf58ywVJFib5WJJvtulJQ/0XJ7kyyXvbZ21BW/bJ9tm6so1oQJJTgfu0z+kHW9+t7fW8JL8/dMyzkzw3yT2SvLUdd3WSl4/9T2JXVFVOu/gE3Npe9wQ+BbwCeCfwxtb/VGBVa38aeFJr379tsxhY0/peApw5tO8+3/b9u639fOC9rX0JsKS1fwv4wqT/TJwm+3kE9gGuAR4A/AVwSlv2IeDJrf0w4OrWPhN4fWs/AyhgQZvfr73eB1gDPGj6OFset70+Gzinte/JYOic+zAYHucvW/+9gJXAwyf95zXXpjn1PQXtsPskWdXaXwbeB3wd+COAqvpCkgcl2Qf4KvC29r+rj1fVumTksVQ+wiAMvsjgi4XvSnJ/4InABUP7udfdf0valVXVzUnOBV4N3Da06PeAQ4Y+K/u0z9CTGfxjTlVdmOTHQ9u8OsmzW/sgYAlw0zYO/2/AGUnuxSBgvlRVtyU5EnhskulLpQ9o+/rBjr7P3ZGhsHu4raqWDnds7R/6qjo1yb8yuG/w1SRPB34+4nFWAH+bZD/gUOALwP2A/93y+BLwj8DlwAeG+vYADq+qO3zmtvZ5TXIEgyB5QlX9LMm/A/fe1kGr6udtvacz+E/MedO7A15VVRdt39uYX7ynsPv6MvAC6H+xftj+93ZwVV1RVX/PYFiRLa//3wLsPdMOq+rWts0ZwGeqanNV3Qz8IMnz2rGS5HHjeEPatVTVj4DzgROGuj8HvGp6JsnS1vwqcEzrOxLYt/U/APhxC4RHAYcP7etXSfbayuE/ArwU+G3gwtZ3EfCK6W2SPDLJ/Xbs3e2+DIXd1ynAoUlWA6cC02NBn9RuKq8GfsXgVHvYFxmc3q9K8vwZ9vsR4IXtddoLgBOSfBu4En8DQ7c7ncGw19NeDUy1G71XcfuTcm8CjkyyBngecD2D/6BcCOyZ5GoGn+NLh/a1HFg9faN5C58DngJ8vga/zQLwXuAq4PJ2nHfj1ZI7cZgLSRPXrv9vrsH4Z08AzvKS5GSYkpLmgocB57dHnn8J/MmE65m3PFOQJHXeU5AkdYaCJKkzFCRJnTeapa1I8iAGQ3gAPATYDGxs84cNPeq4M471QOCPq+pdO2uf0o7wRrM0gjb0861VddoI6+5ZVZu2c/+LGXwh8DE7VqG0c3j5SNoOSf6kjbL57Tba531b/9lJ/jnJ14F/aCPHXprkiiR/Mz2CZ1v3dUMjdb6pdZ8KHNy+NPjWCbw1CTAUpO318ar6zap6HHA1dxzC4UDgiVX1GgZDgZxRVb8OrJteoQ3hsAQ4jMFvVxya5HeAk4H/qsHvYrxudt6KdGeGgrR9HtN+M+IKBsN7PHpo2QVVtbm1nwBc0NofGlrnyDZ9i8FgcY9iEBLSnOCNZmn7nA08q6q+neQlwBFDy346wvYB/q6q3n2HzsE9BWniPFOQts/ewIY20uYLtrHepbTfs2Dw2xPTLgJe1n5DgCSLkjyYbYxOK80mQ0HaPn/F4AeMvgp8ZxvrnQS8po1G+wjgJwBV9TkGl5O+1i5BfRTYu6puYvD7Fmu80axJ8pFUaQzaU0m3VVUlORY4rqocUlxznvcUpPE4FDgzg58U+1/gZZMtRxqNZwqSpM57CpKkzlCQJHWGgiSpMxQkSZ2hIEnq/h/HYzBrG5RY7AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x = \"Target\", data = df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99cdb845-4028-4131-aac9-c28803880a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 25759\n"
     ]
    }
   ],
   "source": [
    "train_data_all, train_label_all, train_target_all = df_train[\"Text\"].tolist(), df_train[\"Sentiment\"].tolist(), df_train['Target'].tolist()\n",
    "dev_data, dev_labels, dev_target = df_val[\"Text\"].tolist(), df_val[\"Sentiment\"].tolist(), df_val['Target'].tolist()\n",
    "data = train_data_all+dev_data\n",
    "labels = train_label_all+dev_labels\n",
    "targets = train_target_all+dev_target\n",
    "pairs = []\n",
    "for c in zip(labels, data, targets):\n",
    "    if len(c[1]) > 0:\n",
    "        pairs.append(c)\n",
    "random.seed(0)\n",
    "random.shuffle(pairs)\n",
    "N_EXAMPLES = len(pairs)\n",
    "print(\"Total: {}\".format(N_EXAMPLES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c9aca13-740c-45ce-a0fb-3151ea717079",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = BobcatParser(verbose='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0229c291-d693-41c5-8374-f6fd3ffaac38",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f656e77c99a4430ab675763396a0380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25759 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_data = []\n",
    "new_label = []\n",
    "new_target = []\n",
    "i = 0 # positive\n",
    "j = 0 # negative\n",
    "for label, sent, target in tqdm(pairs):\n",
    "    try:\n",
    "        diag = parser.sentence2diagram(sent)\n",
    "    except:\n",
    "        pass\n",
    "    else:\n",
    "        sent_length = len(sent.split(\" \"))\n",
    "        if i>round(N_EXAMPLES*TOTAL_DATA_RATIO)//2 and j>round(N_EXAMPLES*TOTAL_DATA_RATIO)//2:\n",
    "            break\n",
    "        if target == \"Positive\" and i<=round(N_EXAMPLES*TOTAL_DATA_RATIO)//2:\n",
    "            new_data.append(sent)\n",
    "            new_label.append(label)\n",
    "            new_target.append(target)\n",
    "            i = i + 1\n",
    "        if target == 'Negative' and j<=round(N_EXAMPLES*TOTAL_DATA_RATIO)//2:\n",
    "            new_data.append(sent)\n",
    "            new_label.append(label)\n",
    "            new_target.append(target)\n",
    "            j = j + 1\n",
    "\n",
    "cleaned_qnlp_data = {\"data\":new_data, \"label\":new_label, \"target\":new_target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e118b9c-6d24-451b-b791-12ff00efacb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(cleaned_qnlp_data, open(\"cleaned_qnlp_data_stem_lematize.pkl\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b65b031-843a-4f18-96c6-cc0561a689c6",
   "metadata": {},
   "source": [
    "**No stemming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7e1bad9-87d9-453b-9645-bbbd6b76e51c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/app/data/twitter_training.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(word_list)\n\u001b[1;32m      7\u001b[0m columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mId\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEntity\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 8\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/app/data/twitter_training.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#data = data.sample(frac=1).reset_index(drop=True)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m data_val \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/app/data/twitter_validation.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, names\u001b[38;5;241m=\u001b[39mcolumns,header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py:934\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 934\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1218\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1214\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[0;32m-> 1218\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1229\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/common.py:786\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 786\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    793\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    794\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/app/data/twitter_training.csv'"
     ]
    }
   ],
   "source": [
    "def get_sent_length(sent):\n",
    "    if type(sent) is not str:\n",
    "        return 9999999999999\n",
    "    word_list = sent.split(\" \")\n",
    "    return len(word_list)\n",
    "\n",
    "columns = [\"Id\",\"Entity\",\"Target\",\"Text\"]\n",
    "data = pd.read_csv(os.path.join(os.getcwd(),\"data/twitter_training.csv\"), names=columns,header=None)\n",
    "#data = data.sample(frac=1).reset_index(drop=True)\n",
    "data_val = pd.read_csv(\"/app/data/twitter_validation.csv\", names=columns,header=None)\n",
    "#data_val = data.sample(frac=1).reset_index(drop=True)\n",
    "df_train = data[[\"Text\",\"Target\"]]\n",
    "df_train = df_train.loc[(df_train[\"Target\"]=='Positive') | (df_train[\"Target\"]=='Negative') & (df_train[\"Text\"]!=np.nan)&(df_train[\"Text\"].map(get_sent_length)<=MAX_LENGTH)]\n",
    "df_train= df_train.drop_duplicates()\n",
    "df_val = data_val[['Text', 'Target']]\n",
    "df_val = df_val.loc[(df_val['Target'] == 'Positive') | (df_val['Target'] == 'Negative') & (df_val[\"Text\"]!=np.nan)&(df_val[\"Text\"].map(get_sent_length)<=MAX_LENGTH)]\n",
    "text_cleaning_re = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "def preprocess(text):\n",
    "    text = re.sub(text_cleaning_re, ' ', str(text).lower()).strip()\n",
    "    \n",
    "    without_emoji = emoji_pattern.sub(r'', text)\n",
    "    \n",
    "    tokens = word_tokenize(str(without_emoji).replace(\"'\", \"\").lower()) \n",
    "    \n",
    "    # Remove Puncs\n",
    "    without_punc = [w for w in tokens if w.isalpha()]\n",
    "    \n",
    "    text_len = [WordNetLemmatizer().lemmatize(t) for t in without_punc]\n",
    "    \n",
    "    return \" \".join(text_len)\n",
    "\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"im\",\"i am\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"i'm\",\"i am\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"I'm\",\"i am\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"it's\",\"it is\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"you're\",\"you are\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"hasn't\",\"has not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"haven't\",\"have not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"don't\",\"do not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"doesn't\",\"does not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"won't\",\"will not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"shouldn't\",\"should not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"can't\",\"can not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"couldn't\",\"could not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"im\",\"i am\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"i'm\",\"i am\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"I'm\",\"i am\")\n",
    "df_val[\"Text\"]= df_val[\"Text\"].str.replace(\"it's\",\"it is\")\n",
    "df_val[\"Text\"]= df_val[\"Text\"].str.replace(\"you're\",\"you are\")\n",
    "df_val[\"Text\"]= df_val[\"Text\"].str.replace(\"hasn't\",\"has not\")\n",
    "df_val[\"Text\"]= df_val[\"Text\"].str.replace(\"haven't\",\"have not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"don't\",\"do not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"doesn't\",\"does not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"won't\",\"will not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"shouldn't\",\"should not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"can't\",\"can not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"couldn't\",\"could not\")\n",
    "df_train[\"Text\"] = df_train[\"Text\"].apply(preprocess)\n",
    "df_val[\"Text\"] = df_val[\"Text\"].apply(preprocess)\n",
    "df_train = df_train.dropna()\n",
    "df_val = df_val.dropna()\n",
    "\n",
    "\n",
    "# Positive sentiment to [0,1], negative sentiment to [1,0]\n",
    "sentiment_train = []\n",
    "sentiment_val = []\n",
    "for i in df_train[\"Target\"]:\n",
    "    if i == \"Positive\":\n",
    "        sentiment_train.append([0,1])\n",
    "    else:\n",
    "        sentiment_train.append([1,0])\n",
    "\n",
    "df_train[\"Sentiment\"] = sentiment_train\n",
    "        \n",
    "for i in df_val[\"Target\"]:\n",
    "    if i == \"Positive\":\n",
    "        sentiment_val.append([0,1])\n",
    "    else:\n",
    "        sentiment_val.append([1,0])\n",
    "\n",
    "df_val[\"Sentiment\"] = sentiment_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8705ee-15f3-4f4e-ac20-f77974c72320",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_all, train_label_all, train_target_all = df_train[\"Text\"].tolist(), df_train[\"Sentiment\"].tolist(), df_train['Target'].tolist()\n",
    "dev_data, dev_labels, dev_target = df_val[\"Text\"].tolist(), df_val[\"Sentiment\"].tolist(), df_val['Target'].tolist()\n",
    "data = train_data_all+dev_data\n",
    "labels = train_label_all+dev_labels\n",
    "targets = train_target_all+dev_target\n",
    "pairs = []\n",
    "for c in zip(labels, data, targets):\n",
    "    if len(c[1]) > 0:\n",
    "        pairs.append(c)\n",
    "random.seed(0)\n",
    "random.shuffle(pairs)\n",
    "N_EXAMPLES = len(pairs)\n",
    "print(\"Total: {}\".format(N_EXAMPLES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28da51ba-74be-44f7-b6fd-4c52265a7615",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = []\n",
    "new_label = []\n",
    "new_target = []\n",
    "i = 0 # positive\n",
    "j = 0 # negative\n",
    "for label, sent, target in tqdm(pairs):\n",
    "    try:\n",
    "        diag = parser.sentence2diagram(sent)\n",
    "    except:\n",
    "        pass\n",
    "    else:\n",
    "        sent_length = len(sent.split(\" \"))\n",
    "        if target == \"Positive\" and i<=round(N_EXAMPLES*TOTAL_DATA_RATIO)//2:\n",
    "            new_data.append(sent)\n",
    "            new_label.append(label)\n",
    "            new_target.append(target)\n",
    "            i = i + 1\n",
    "        if target == 'Negative' and j<=round(N_EXAMPLES*TOTAL_DATA_RATIO)//2:\n",
    "            new_data.append(sent)\n",
    "            new_label.append(label)\n",
    "            new_target.append(target)\n",
    "            j = j + 1\n",
    "        \n",
    "cleaned_qnlp_data = {\"data\":new_data, \"label\":new_label, \"target\":new_target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18cf5c6-a1bd-4fab-8277-58d0f19e3c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(cleaned_qnlp_data, open(\"cleaned_qnlp_data_lematize.pkl\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6997ad74-aa25-4622-bd27-6a724982d394",
   "metadata": {},
   "source": [
    "**Without stemming or lematization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7a32eb-6539-4c87-841c-f03b21e2c9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sent_length(sent):\n",
    "    if type(sent) is not str:\n",
    "        return 9999999999999\n",
    "    word_list = sent.split(\" \")\n",
    "    return len(word_list)\n",
    "\n",
    "columns = [\"Id\",\"Entity\",\"Target\",\"Text\"]\n",
    "data = pd.read_csv(\"/app/data/twitter_training.csv\", names=columns,header=None)\n",
    "#data = data.sample(frac=1).reset_index(drop=True)\n",
    "data_val = pd.read_csv(\"/app/data/twitter_validation.csv\", names=columns,header=None)\n",
    "#data_val = data.sample(frac=1).reset_index(drop=True)\n",
    "df_train = data[[\"Text\",\"Target\"]]\n",
    "df_train = df_train.loc[(df_train[\"Target\"]=='Positive') | (df_train[\"Target\"]=='Negative') & (df_train[\"Text\"]!=np.nan)&(df_train[\"Text\"].map(get_sent_length)<=MAX_LENGTH)]\n",
    "df_train= df_train.drop_duplicates()\n",
    "df_val = data_val[['Text', 'Target']]\n",
    "df_val = df_val.loc[(df_val['Target'] == 'Positive') | (df_val['Target'] == 'Negative') & (df_val[\"Text\"]!=np.nan)&(df_val[\"Text\"].map(get_sent_length)<=MAX_LENGTH)]\n",
    "text_cleaning_re = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "def preprocess(text):\n",
    "    text = re.sub(text_cleaning_re, ' ', str(text).lower()).strip()\n",
    "    \n",
    "    without_emoji = emoji_pattern.sub(r'', text)\n",
    "    \n",
    "    tokens = word_tokenize(str(without_emoji).replace(\"'\", \"\").lower()) \n",
    "    \n",
    "    # Remove Puncs\n",
    "    without_punc = [w for w in tokens if w.isalpha()]\n",
    "    \n",
    "    # text_len = [WordNetLemmatizer().lemmatize(t) for t in without_punc]\n",
    "    \n",
    "    return \" \".join(without_punc)\n",
    "\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"im\",\"i am\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"i'm\",\"i am\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"I'm\",\"i am\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"it's\",\"it is\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"you're\",\"you are\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"hasn't\",\"has not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"haven't\",\"have not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"don't\",\"do not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"doesn't\",\"does not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"won't\",\"will not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"shouldn't\",\"should not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"can't\",\"can not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"couldn't\",\"could not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"im\",\"i am\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"i'm\",\"i am\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"I'm\",\"i am\")\n",
    "df_val[\"Text\"]= df_val[\"Text\"].str.replace(\"it's\",\"it is\")\n",
    "df_val[\"Text\"]= df_val[\"Text\"].str.replace(\"you're\",\"you are\")\n",
    "df_val[\"Text\"]= df_val[\"Text\"].str.replace(\"hasn't\",\"has not\")\n",
    "df_val[\"Text\"]= df_val[\"Text\"].str.replace(\"haven't\",\"have not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"don't\",\"do not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"doesn't\",\"does not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"won't\",\"will not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"shouldn't\",\"should not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"can't\",\"can not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"couldn't\",\"could not\")\n",
    "df_train[\"Text\"] = df_train[\"Text\"].apply(preprocess)\n",
    "df_val[\"Text\"] = df_val[\"Text\"].apply(preprocess)\n",
    "df_train = df_train.dropna()\n",
    "df_val = df_val.dropna()\n",
    "\n",
    "\n",
    "# Positive sentiment to [0,1], negative sentiment to [1,0]\n",
    "sentiment_train = []\n",
    "sentiment_val = []\n",
    "for i in df_train[\"Target\"]:\n",
    "    if i == \"Positive\":\n",
    "        sentiment_train.append([0,1])\n",
    "    else:\n",
    "        sentiment_train.append([1,0])\n",
    "\n",
    "df_train[\"Sentiment\"] = sentiment_train\n",
    "        \n",
    "for i in df_val[\"Target\"]:\n",
    "    if i == \"Positive\":\n",
    "        sentiment_val.append([0,1])\n",
    "    else:\n",
    "        sentiment_val.append([1,0])\n",
    "\n",
    "df_val[\"Sentiment\"] = sentiment_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ed0cd2-f297-461e-bb0f-d282b0e83dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_all, train_label_all, train_target_all = df_train[\"Text\"].tolist(), df_train[\"Sentiment\"].tolist(), df_train['Target'].tolist()\n",
    "dev_data, dev_labels, dev_target = df_val[\"Text\"].tolist(), df_val[\"Sentiment\"].tolist(), df_val['Target'].tolist()\n",
    "data = train_data_all+dev_data\n",
    "labels = train_label_all+dev_labels\n",
    "targets = train_target_all+dev_target\n",
    "pairs = []\n",
    "for c in zip(labels, data, targets):\n",
    "    if len(c[1]) > 0:\n",
    "        pairs.append(c)\n",
    "random.seed(0)\n",
    "random.shuffle(pairs)\n",
    "N_EXAMPLES = len(pairs)\n",
    "print(\"Total: {}\".format(N_EXAMPLES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e799c02-c0ad-44c5-9f7e-5ea658b165ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = []\n",
    "new_label = []\n",
    "new_target = []\n",
    "i = 0 # positive\n",
    "j = 0 # negative\n",
    "for label, sent, target in tqdm(pairs):\n",
    "    try:\n",
    "        diag = parser.sentence2diagram(sent)\n",
    "    except:\n",
    "        pass\n",
    "    else:\n",
    "        sent_length = len(sent.split(\" \"))\n",
    "        if i>round(N_EXAMPLES*TOTAL_DATA_RATIO)//2 and j>round(N_EXAMPLES*TOTAL_DATA_RATIO)//2:\n",
    "            break\n",
    "        if target == \"Positive\" and i<=round(N_EXAMPLES*TOTAL_DATA_RATIO)//2:\n",
    "            new_data.append(sent)\n",
    "            new_label.append(label)\n",
    "            new_target.append(target)\n",
    "            i = i + 1\n",
    "        if target == 'Negative' and j<=round(N_EXAMPLES*TOTAL_DATA_RATIO)//2:\n",
    "            new_data.append(sent)\n",
    "            new_label.append(label)\n",
    "            new_target.append(target)\n",
    "            j = j + 1\n",
    "\n",
    "cleaned_qnlp_data = {\"data\":new_data, \"label\":new_label, \"target\":new_target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496963db-a3b9-49f2-826c-b33b7c3ee98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(cleaned_qnlp_data, open(\"cleaned_qnlp_data.pkl\", \"wb\" ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
