{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cf997fd-8f97-4317-bef6-16a8c5c6ebc0",
   "metadata": {},
   "source": [
    "# Preprocess data for QNLP pipeline\n",
    "Remove all the sentences that cannot be parsed by BobcatParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bf24063-0233-4278-b55f-73ede0cd0a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.9/13.9 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /opt/conda/lib/python3.9/site-packages (from en-core-web-sm==3.2.0) (3.2.4)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: click<8.1.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.4)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.7)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (61.2.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.1.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.21.5)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.3)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.17)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.27.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.9)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.9.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.64.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /opt/conda/lib/python3.9/site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2021.5.30)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.10)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.9/site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d2fd85e-7513-40c6-b395-1d253060b594",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /root/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/app\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "from random import shuffle\n",
    "import re\n",
    "import spacy\n",
    "from discopy.tensor import Tensor\n",
    "from discopy import Word\n",
    "from discopy.rigid import Functor\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import random, unique\n",
    "from lambeq import AtomicType, IQPAnsatz, remove_cups, NumpyModel, spiders_reader\n",
    "from lambeq import BobcatParser, TreeReader, cups_reader, DepCCGParser\n",
    "from lambeq import Dataset\n",
    "from lambeq import QuantumTrainer, SPSAOptimizer\n",
    "from lambeq import TketModel\n",
    "from lambeq import SpacyTokeniser\n",
    "from pytket.extensions.qiskit import AerBackend\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk import pos_tag, ne_chunk\n",
    "from nltk.chunk import tree2conlltags\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.options.display.max_colwidth=80\n",
    "print(os.getcwd())\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "spacy.load('en_core_web_sm')\n",
    "\n",
    "TOTAL_DATA_RATIO = 0.1 # only use part of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4624e0-410d-4635-9bef-f4bbead799ec",
   "metadata": {},
   "source": [
    "**With stemming and lematization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b5fed62-abb8-4e9d-9c76-bcaba7e66bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sent_length(sent):\n",
    "    if type(sent) is not str:\n",
    "        return 9999999999999\n",
    "    word_list = sent.split(\" \")\n",
    "    return len(word_list)\n",
    "\n",
    "columns = [\"Id\",\"Entity\",\"Target\",\"Text\"]\n",
    "data = pd.read_csv(\"/app/data/twitter_training.csv\", names=columns,header=None)\n",
    "#data = data.sample(frac=1).reset_index(drop=True)\n",
    "data_val = pd.read_csv(\"/app/data/twitter_validation.csv\", names=columns,header=None)\n",
    "#data_val = data.sample(frac=1).reset_index(drop=True)\n",
    "df_train = data[[\"Text\",\"Target\"]]\n",
    "df_train = df_train.loc[(df_train[\"Target\"]=='Positive') | (df_train[\"Target\"]=='Negative') & (df_train[\"Text\"]!=np.nan)&(df_train[\"Text\"].map(get_sent_length)<=999999)]\n",
    "df_train= df_train.drop_duplicates()\n",
    "df_val = data_val[['Text', 'Target']]\n",
    "df_val = df_val.loc[(df_val['Target'] == 'Positive') | (df_val['Target'] == 'Negative') & (df_val[\"Text\"]!=np.nan)&(df_val[\"Text\"].map(get_sent_length)<=999999)]\n",
    "text_cleaning_re = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "def preprocess(text):\n",
    "    text = re.sub(text_cleaning_re, ' ', str(text).lower()).strip()\n",
    "    \n",
    "    without_emoji = emoji_pattern.sub(r'', text)\n",
    "    \n",
    "    tokens = word_tokenize(str(without_emoji).replace(\"'\", \"\").lower()) \n",
    "    \n",
    "    # Remove Puncs\n",
    "    without_punc = [w for w in tokens if w.isalpha()]\n",
    "    \n",
    "    # Lemmatize\n",
    "    text_len = [WordNetLemmatizer().lemmatize(t) for t in without_punc]\n",
    "    # Stem\n",
    "    text_cleaned = [PorterStemmer().stem(w) for w in text_len]\n",
    "    \n",
    "    return \" \".join(text_cleaned)\n",
    "\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"im\",\"i am\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"i'm\",\"i am\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"I'm\",\"i am\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"it's\",\"it is\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"you're\",\"you are\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"hasn't\",\"has not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"haven't\",\"have not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"don't\",\"do not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"doesn't\",\"does not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"won't\",\"will not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"shouldn't\",\"should not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"can't\",\"can not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"couldn't\",\"could not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"im\",\"i am\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"i'm\",\"i am\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"I'm\",\"i am\")\n",
    "df_val[\"Text\"]= df_val[\"Text\"].str.replace(\"it's\",\"it is\")\n",
    "df_val[\"Text\"]= df_val[\"Text\"].str.replace(\"you're\",\"you are\")\n",
    "df_val[\"Text\"]= df_val[\"Text\"].str.replace(\"hasn't\",\"has not\")\n",
    "df_val[\"Text\"]= df_val[\"Text\"].str.replace(\"haven't\",\"have not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"don't\",\"do not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"doesn't\",\"does not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"won't\",\"will not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"shouldn't\",\"should not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"can't\",\"can not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"couldn't\",\"could not\")\n",
    "df_train[\"Text\"] = df_train[\"Text\"].apply(preprocess)\n",
    "df_val[\"Text\"] = df_val[\"Text\"].apply(preprocess)\n",
    "df_train = df_train.dropna()\n",
    "df_val = df_val.dropna()\n",
    "\n",
    "\n",
    "# Positive sentiment to [0,1], negative sentiment to [1,0]\n",
    "sentiment_train = []\n",
    "sentiment_val = []\n",
    "for i in df_train[\"Target\"]:\n",
    "    if i == \"Positive\":\n",
    "        sentiment_train.append([0,1])\n",
    "    else:\n",
    "        sentiment_train.append([1,0])\n",
    "\n",
    "df_train[\"Sentiment\"] = sentiment_train\n",
    "        \n",
    "for i in df_val[\"Target\"]:\n",
    "    if i == \"Positive\":\n",
    "        sentiment_val.append([0,1])\n",
    "    else:\n",
    "        sentiment_val.append([1,0])\n",
    "\n",
    "df_val[\"Sentiment\"] = sentiment_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb019d34-b2dc-447b-9712-9696a872e361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 40376 entries, 0 to 74681\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   Text       40376 non-null  object\n",
      " 1   Target     40376 non-null  object\n",
      " 2   Sentiment  40376 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaf01964-3510-4b52-af89-38f5177e39fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 543 entries, 2 to 998\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   Text       543 non-null    object\n",
      " 1   Target     543 non-null    object\n",
      " 2   Sentiment  543 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 17.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df_val.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52fdbf5b-7501-404e-b04f-264ccea51ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Target</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i am get on borderland and i will murder you all</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i am come to the border and i will kill you all</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i am get on borderland and i will kill you all</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am come on borderland and i will murder you all</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am get on borderland and i will murder you me all</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text    Target Sentiment\n",
       "0     i am get on borderland and i will murder you all  Positive    [0, 1]\n",
       "1      i am come to the border and i will kill you all  Positive    [0, 1]\n",
       "2       i am get on borderland and i will kill you all  Positive    [0, 1]\n",
       "3    i am come on borderland and i will murder you all  Positive    [0, 1]\n",
       "4  i am get on borderland and i will murder you me all  Positive    [0, 1]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1de2be1-bd00-47eb-9f77-590dfe775ad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Target</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>whi do i pay for word when it function so poorli on my samsungu chromebook</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>csgo matchmak is so full of closet hack it is a truli aw game</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hi eahelp i ve had madelein mccann in my cellar for the past year and the li...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>thank you eamaddennfl new te austin hooper in the orang brown brown pic twit...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rocket leagu sea of thief or rainbow six sieg i love play all three on strea...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                              Text    Target Sentiment\n",
       "2       whi do i pay for word when it function so poorli on my samsungu chromebook  Negative    [1, 0]\n",
       "3                    csgo matchmak is so full of closet hack it is a truli aw game  Negative    [1, 0]\n",
       "5  hi eahelp i ve had madelein mccann in my cellar for the past year and the li...  Negative    [1, 0]\n",
       "6  thank you eamaddennfl new te austin hooper in the orang brown brown pic twit...  Positive    [0, 1]\n",
       "7  rocket leagu sea of thief or rainbow six sieg i love play all three on strea...  Positive    [0, 1]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99cdb845-4028-4131-aac9-c28803880a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 40840\n"
     ]
    }
   ],
   "source": [
    "train_data_all, train_label_all, train_target_all = df_train[\"Text\"].tolist(), df_train[\"Sentiment\"].tolist(), df_train['Target'].tolist()\n",
    "dev_data, dev_labels, dev_target = df_val[\"Text\"].tolist(), df_val[\"Sentiment\"].tolist(), df_val['Target'].tolist()\n",
    "data = train_data_all+dev_data\n",
    "labels = train_label_all+dev_labels\n",
    "targets = train_target_all+dev_target\n",
    "pairs = []\n",
    "for c in zip(labels, data, targets):\n",
    "    if len(c[1]) > 0:\n",
    "        pairs.append(c)\n",
    "random.seed(0)\n",
    "random.shuffle(pairs)\n",
    "N_EXAMPLES = len(pairs)\n",
    "print(\"Total: {}\".format(N_EXAMPLES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c9aca13-740c-45ce-a0fb-3151ea717079",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = BobcatParser(verbose='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0229c291-d693-41c5-8374-f6fd3ffaac38",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26062047ff2c46e193fb1369149c7c18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40840 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_data = []\n",
    "new_label = []\n",
    "new_target = []\n",
    "i = 0\n",
    "for label, sent, target in tqdm(pairs):\n",
    "    try:\n",
    "        diag = parser.sentence2diagram(sent)\n",
    "    except:\n",
    "        pass\n",
    "    else:\n",
    "        new_data.append(sent)\n",
    "        new_label.append(label)\n",
    "        new_target.append(target)\n",
    "        i = i + 1\n",
    "        if i>=round(N_EXAMPLES*TOTAL_DATA_RATIO):\n",
    "            break\n",
    "\n",
    "cleaned_qnlp_data = {\"data\":new_data, \"label\":new_label, \"target\":new_target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e118b9c-6d24-451b-b791-12ff00efacb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(cleaned_qnlp_data, open(\"cleaned_qnlp_data_stem_lematize.pkl\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b65b031-843a-4f18-96c6-cc0561a689c6",
   "metadata": {},
   "source": [
    "**No stemming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e1bad9-87d9-453b-9645-bbbd6b76e51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sent_length(sent):\n",
    "    if type(sent) is not str:\n",
    "        return 9999999999999\n",
    "    word_list = sent.split(\" \")\n",
    "    return len(word_list)\n",
    "\n",
    "columns = [\"Id\",\"Entity\",\"Target\",\"Text\"]\n",
    "data = pd.read_csv(\"/app/data/twitter_training.csv\", names=columns,header=None)\n",
    "#data = data.sample(frac=1).reset_index(drop=True)\n",
    "data_val = pd.read_csv(\"/app/data/twitter_validation.csv\", names=columns,header=None)\n",
    "#data_val = data.sample(frac=1).reset_index(drop=True)\n",
    "df_train = data[[\"Text\",\"Target\"]]\n",
    "df_train = df_train.loc[(df_train[\"Target\"]=='Positive') | (df_train[\"Target\"]=='Negative') & (df_train[\"Text\"]!=np.nan)&(df_train[\"Text\"].map(get_sent_length)<=999999)]\n",
    "df_train= df_train.drop_duplicates()\n",
    "df_val = data_val[['Text', 'Target']]\n",
    "df_val = df_val.loc[(df_val['Target'] == 'Positive') | (df_val['Target'] == 'Negative') & (df_val[\"Text\"]!=np.nan)&(df_val[\"Text\"].map(get_sent_length)<=999999)]\n",
    "text_cleaning_re = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "def preprocess(text):\n",
    "    text = re.sub(text_cleaning_re, ' ', str(text).lower()).strip()\n",
    "    \n",
    "    without_emoji = emoji_pattern.sub(r'', text)\n",
    "    \n",
    "    tokens = word_tokenize(str(without_emoji).replace(\"'\", \"\").lower()) \n",
    "    \n",
    "    # Remove Puncs\n",
    "    without_punc = [w for w in tokens if w.isalpha()]\n",
    "    \n",
    "    text_len = [WordNetLemmatizer().lemmatize(t) for t in without_punc]\n",
    "    \n",
    "    return \" \".join(text_len)\n",
    "\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"im\",\"i am\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"i'm\",\"i am\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"I'm\",\"i am\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"it's\",\"it is\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"you're\",\"you are\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"hasn't\",\"has not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"haven't\",\"have not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"don't\",\"do not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"doesn't\",\"does not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"won't\",\"will not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"shouldn't\",\"should not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"can't\",\"can not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"couldn't\",\"could not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"im\",\"i am\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"i'm\",\"i am\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"I'm\",\"i am\")\n",
    "df_val[\"Text\"]= df_val[\"Text\"].str.replace(\"it's\",\"it is\")\n",
    "df_val[\"Text\"]= df_val[\"Text\"].str.replace(\"you're\",\"you are\")\n",
    "df_val[\"Text\"]= df_val[\"Text\"].str.replace(\"hasn't\",\"has not\")\n",
    "df_val[\"Text\"]= df_val[\"Text\"].str.replace(\"haven't\",\"have not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"don't\",\"do not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"doesn't\",\"does not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"won't\",\"will not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"shouldn't\",\"should not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"can't\",\"can not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"couldn't\",\"could not\")\n",
    "df_train[\"Text\"] = df_train[\"Text\"].apply(preprocess)\n",
    "df_val[\"Text\"] = df_val[\"Text\"].apply(preprocess)\n",
    "df_train = df_train.dropna()\n",
    "df_val = df_val.dropna()\n",
    "\n",
    "\n",
    "# Positive sentiment to [0,1], negative sentiment to [1,0]\n",
    "sentiment_train = []\n",
    "sentiment_val = []\n",
    "for i in df_train[\"Target\"]:\n",
    "    if i == \"Positive\":\n",
    "        sentiment_train.append([0,1])\n",
    "    else:\n",
    "        sentiment_train.append([1,0])\n",
    "\n",
    "df_train[\"Sentiment\"] = sentiment_train\n",
    "        \n",
    "for i in df_val[\"Target\"]:\n",
    "    if i == \"Positive\":\n",
    "        sentiment_val.append([0,1])\n",
    "    else:\n",
    "        sentiment_val.append([1,0])\n",
    "\n",
    "df_val[\"Sentiment\"] = sentiment_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8705ee-15f3-4f4e-ac20-f77974c72320",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_all, train_label_all, train_target_all = df_train[\"Text\"].tolist(), df_train[\"Sentiment\"].tolist(), df_train['Target'].tolist()\n",
    "dev_data, dev_labels, dev_target = df_val[\"Text\"].tolist(), df_val[\"Sentiment\"].tolist(), df_val['Target'].tolist()\n",
    "data = train_data_all+dev_data\n",
    "labels = train_label_all+dev_labels\n",
    "targets = train_target_all+dev_target\n",
    "pairs = []\n",
    "for c in zip(labels, data, targets):\n",
    "    if len(c[1]) > 0:\n",
    "        pairs.append(c)\n",
    "random.seed(0)\n",
    "random.shuffle(pairs)\n",
    "N_EXAMPLES = len(pairs)\n",
    "print(\"Total: {}\".format(N_EXAMPLES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28da51ba-74be-44f7-b6fd-4c52265a7615",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = []\n",
    "new_label = []\n",
    "new_target = []\n",
    "i = 0\n",
    "for label, sent, target in tqdm(pairs):\n",
    "    try:\n",
    "        diag = parser.sentence2diagram(sent)\n",
    "    except:\n",
    "        pass\n",
    "    else:\n",
    "        new_data.append(sent)\n",
    "        new_label.append(label)\n",
    "        new_target.append(target)\n",
    "        i = i + 1\n",
    "        if i>=round(N_EXAMPLES*TOTAL_DATA_RATIO):\n",
    "            break\n",
    "\n",
    "cleaned_qnlp_data = {\"data\":new_data, \"label\":new_label, \"target\":new_target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18cf5c6-a1bd-4fab-8277-58d0f19e3c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(cleaned_qnlp_data, open(\"cleaned_qnlp_data_lematize.pkl\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6997ad74-aa25-4622-bd27-6a724982d394",
   "metadata": {},
   "source": [
    "**Without stemming or lematization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7a32eb-6539-4c87-841c-f03b21e2c9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sent_length(sent):\n",
    "    if type(sent) is not str:\n",
    "        return 9999999999999\n",
    "    word_list = sent.split(\" \")\n",
    "    return len(word_list)\n",
    "\n",
    "columns = [\"Id\",\"Entity\",\"Target\",\"Text\"]\n",
    "data = pd.read_csv(\"/app/data/twitter_training.csv\", names=columns,header=None)\n",
    "#data = data.sample(frac=1).reset_index(drop=True)\n",
    "data_val = pd.read_csv(\"/app/data/twitter_validation.csv\", names=columns,header=None)\n",
    "#data_val = data.sample(frac=1).reset_index(drop=True)\n",
    "df_train = data[[\"Text\",\"Target\"]]\n",
    "df_train = df_train.loc[(df_train[\"Target\"]=='Positive') | (df_train[\"Target\"]=='Negative') & (df_train[\"Text\"]!=np.nan)&(df_train[\"Text\"].map(get_sent_length)<=999999)]\n",
    "df_train= df_train.drop_duplicates()\n",
    "df_val = data_val[['Text', 'Target']]\n",
    "df_val = df_val.loc[(df_val['Target'] == 'Positive') | (df_val['Target'] == 'Negative') & (df_val[\"Text\"]!=np.nan)&(df_val[\"Text\"].map(get_sent_length)<=999999)]\n",
    "text_cleaning_re = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "def preprocess(text):\n",
    "    text = re.sub(text_cleaning_re, ' ', str(text).lower()).strip()\n",
    "    \n",
    "    without_emoji = emoji_pattern.sub(r'', text)\n",
    "    \n",
    "    tokens = word_tokenize(str(without_emoji).replace(\"'\", \"\").lower()) \n",
    "    \n",
    "    # Remove Puncs\n",
    "    without_punc = [w for w in tokens if w.isalpha()]\n",
    "    \n",
    "    text_len = [WordNetLemmatizer().lemmatize(t) for t in without_punc]\n",
    "    \n",
    "    return \" \".join(text_len)\n",
    "\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"im\",\"i am\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"i'm\",\"i am\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"I'm\",\"i am\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"it's\",\"it is\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"you're\",\"you are\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"hasn't\",\"has not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"haven't\",\"have not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"don't\",\"do not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"doesn't\",\"does not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"won't\",\"will not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"shouldn't\",\"should not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"can't\",\"can not\")\n",
    "df_train[\"Text\"]= df_train[\"Text\"].str.replace(\"couldn't\",\"could not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"im\",\"i am\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"i'm\",\"i am\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"I'm\",\"i am\")\n",
    "df_val[\"Text\"]= df_val[\"Text\"].str.replace(\"it's\",\"it is\")\n",
    "df_val[\"Text\"]= df_val[\"Text\"].str.replace(\"you're\",\"you are\")\n",
    "df_val[\"Text\"]= df_val[\"Text\"].str.replace(\"hasn't\",\"has not\")\n",
    "df_val[\"Text\"]= df_val[\"Text\"].str.replace(\"haven't\",\"have not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"don't\",\"do not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"doesn't\",\"does not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"won't\",\"will not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"shouldn't\",\"should not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"can't\",\"can not\")\n",
    "df_val[\"Text\"] = df_val[\"Text\"].str.replace(\"couldn't\",\"could not\")\n",
    "df_train[\"Text\"] = df_train[\"Text\"].apply(preprocess)\n",
    "df_val[\"Text\"] = df_val[\"Text\"].apply(preprocess)\n",
    "df_train = df_train.dropna()\n",
    "df_val = df_val.dropna()\n",
    "\n",
    "\n",
    "# Positive sentiment to [0,1], negative sentiment to [1,0]\n",
    "sentiment_train = []\n",
    "sentiment_val = []\n",
    "for i in df_train[\"Target\"]:\n",
    "    if i == \"Positive\":\n",
    "        sentiment_train.append([0,1])\n",
    "    else:\n",
    "        sentiment_train.append([1,0])\n",
    "\n",
    "df_train[\"Sentiment\"] = sentiment_train\n",
    "        \n",
    "for i in df_val[\"Target\"]:\n",
    "    if i == \"Positive\":\n",
    "        sentiment_val.append([0,1])\n",
    "    else:\n",
    "        sentiment_val.append([1,0])\n",
    "\n",
    "df_val[\"Sentiment\"] = sentiment_val\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ed0cd2-f297-461e-bb0f-d282b0e83dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_all, train_label_all, train_target_all = df_train[\"Text\"].tolist(), df_train[\"Sentiment\"].tolist(), df_train['Target'].tolist()\n",
    "dev_data, dev_labels, dev_target = df_val[\"Text\"].tolist(), df_val[\"Sentiment\"].tolist(), df_val['Target'].tolist()\n",
    "data = train_data_all+dev_data\n",
    "labels = train_label_all+dev_labels\n",
    "targets = train_target_all+dev_target\n",
    "pairs = []\n",
    "for c in zip(labels, data, targets):\n",
    "    if len(c[1]) > 0:\n",
    "        pairs.append(c)\n",
    "random.seed(0)\n",
    "random.shuffle(pairs)\n",
    "N_EXAMPLES = len(pairs)\n",
    "print(\"Total: {}\".format(N_EXAMPLES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e799c02-c0ad-44c5-9f7e-5ea658b165ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = []\n",
    "new_label = []\n",
    "new_target = []\n",
    "i = 0\n",
    "for label, sent, target in tqdm(pairs):\n",
    "    try:\n",
    "        diag = parser.sentence2diagram(sent)\n",
    "    except:\n",
    "        pass\n",
    "    else:\n",
    "        new_data.append(sent)\n",
    "        new_label.append(label)\n",
    "        new_target.append(target)\n",
    "        i = i + 1\n",
    "        if i>=round(N_EXAMPLES*TOTAL_DATA_RATIO):\n",
    "            break\n",
    "\n",
    "cleaned_qnlp_data = {\"data\":new_data, \"label\":new_label, \"target\":new_target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496963db-a3b9-49f2-826c-b33b7c3ee98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(cleaned_qnlp_data, open(\"cleaned_qnlp_data.pkl\", \"wb\" ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
